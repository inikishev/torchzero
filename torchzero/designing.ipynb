{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API CHANGE - store param_groups in var\n",
    "- Modular creates var with it's param_groups\n",
    "- passes var to each module sequentially\n",
    "- if a module has custom param_groups, temporarily set them to var\n",
    "\t- this also has an effect that all child modules will have the same param_groups. That seems like expected behaviord\n",
    "- defaults are handled naturally\n",
    "- projections - they set projected param_groups to var with either fake or real projected params. Params need to be stored in the same Tensor objects for states to work (as projection aleady does). A projected module steps with projected var, no initialization logic is needed. Then unproject, empty projected params and return var\n",
    "- Wrap - can initialize wrapped optimizer to var.param_groups on first step. Need to make sure that if there are no custom parameters, per-parameter settings should be cleared (or just lr? Currently I have just lr, maybe that is better)\n",
    "- Modules are no longer tied to param_groups, although they still store a per-parameter state.\n",
    "- Might also add state to var. But having param_groups in var and state in both self and var can be confusing, so it might need a different name, maybe persistent_state? So the only purpose for this is if I wanted to add variables support, but I am not sure if that is needed. If I ever need variables then I add this\n",
    "\n",
    "- option 3 - same as option 2 but set param_groups object to the module and then delete it after stepping.\n",
    "\t- state_vals don't need params, more consistent with group_vals\n",
    "\t- param_groups is now on self, more consistent with state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 2, 'a': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from collections import UserDict\n",
    "class ParamGroup(UserDict[str, Any]):\n",
    "    __slots__ = ('defaults', )\n",
    "    def __init__(self, group: dict[str, Any], defaults: dict[str, Any]):\n",
    "        super().__init__(group)\n",
    "        self.defaults = defaults\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        if k in self.data: return self.data[k]\n",
    "        return self.defaults[k]\n",
    "\n",
    "    def keys(self): return (self.defaults | self.data).keys()\n",
    "    def values(self): return (self.defaults | self.data).values()\n",
    "    def items(self): return (self.defaults | self.data).items()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return dict.__repr__(self.defaults | self.data)\n",
    "\n",
    "z = ParamGroup({\"a\": 1}, {\"b\": 2, \"a\": 10})\n",
    "dict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('b', 2), ('a', 1)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.items() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "from collections.abc import Iterable, Sequence, Mapping\n",
    "from abc import ABC, abstractmethod\n",
    "from torchzero.utils.optimizer import ParamFilter, ListLike, _param_filter\n",
    "from torchzero.utils import TensorList\n",
    "import torch\n",
    "Params = Iterable[torch.Tensor | tuple[str, torch.Tensor] | Sequence | Mapping[str, Any]]\n",
    "\n",
    "def _make_param_groups(params) -> list[dict[str, Any]]: ...\n",
    "def maybe_chain(modules) -> \"Module\": ...\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self, defaults: dict[str, Any] | None = None):\n",
    "        if defaults is None: defaults = {}\n",
    "        self.defaults: dict[str, Any] = defaults\n",
    "\n",
    "        # this is now temporarily set before stepping\n",
    "        self.param_groups: list[dict[str, Any]] = []\n",
    "\n",
    "        self.state: defaultdict[Any, dict[str, Any]] = defaultdict(dict)\n",
    "        self.global_state: dict[str, Any] = {}\n",
    "\n",
    "        self.children: dict[str, Module] = {}\n",
    "\n",
    "        self._custom_param_groups: list[dict[str, Any]] = []\n",
    "\n",
    "        # initialization logic is not needed anymore?\n",
    "        # self._initialized = False\n",
    "\n",
    "    # not needed methods\n",
    "    # def _initialize(self, params: Params): ...\n",
    "    def set_params(self, params: Params):\n",
    "        self._custom_param_groups = _make_param_groups(params)\n",
    "\n",
    "    def get_params(self, mode: ParamFilter = 'requires_grad', cls: type[ListLike] = TensorList) -> ListLike:\n",
    "        #if not self._initialized: raise RuntimeError(f\"Calling get_params on {self} which is not initialized\")\n",
    "\n",
    "        return cls(p for g in self.param_groups for p in g['params'] if _param_filter(p, mode)) # type:ignore\n",
    "\n",
    "    def set_child(self, key: str, module: \"Module | Iterable[Module]\"):\n",
    "        # not needed\n",
    "        # if self._initialized: raise RuntimeError(f'{self} is already initialized, but trying to set `{key}` child to {module}')\n",
    "\n",
    "        # from .chain import maybe_chain\n",
    "        self.children[key] = maybe_chain(module)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
