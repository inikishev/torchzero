Implementing new modules
############################

Getting started
================================
Implementing torchzero modules is very similar to implementing pytorch optimizers.

First, create a new class inheriting from :py:mod:`tz.core.OptimizerModule<torchzero.core.OptimizerModule>`.

Like in pytorch, putting all settings into :code:`defaults` dictionary allows to specify them as per-parameter options (see :ref:`How to specify per-parameter options?`).

.. code:: python

    import torch, torchzero as tz

    class Adam(tz.core.OptimizerModule):
        def __init__(self, beta1 = 0.9, beta2 = 0.99, eps = 1e-8, alpha = 1):
            defaults = dict(beta1 = beta1, beta2 = beta2, eps = eps)
            super().__init__(defaults)


Note: please don't use :code:`lr` setting in your modules. When learning rate is part of the update rule, like in Adam, I rename it to :code:`alpha` and set to 1 by default. Learning rate should be controlled by a separate :py:class:`tz.m.LR<torchzero.modules.LR>` module, this avoids unintended compounding of learning rate modifications when using learning rate schedulers and per-parameter lr settings (see :ref:`How do we handle learning rates?`).

Implementing the update rule
=============================
Update logic in :code:`OptimizerModule` is defined in the :code:`step` method. By default it calls :code:`_update`, which in turn calls :code:`_single_tensor_update`. You can overwrite one of those three methods depending on how much control you need.

Method 1. Overwriting _single_tensor_update
+++++++++++++++++++++++++++++++++++++++++++++

For most update rules overwriting `_single_tensor_update` is the most convenient way. It allows you to define update rule for a single tensor, and it is then looped over all model parameters.

:code:`_single_tensor_update` accepts the following arguments:

* :code:`vars`: :py:mod:`tz.core.OptimizationVars<torchzero.core.OptimizationVars>` object with various useful attributes, such as closure, list of current update tensors, loss, list of gradient tensors. For now we don't need this.
* :code:`ascent`: torch.Tensor with the ascent direction (update), which is the gradient if this module is first, or an update generated by previous module. This is what the update rule should modify and return.
* :code:`param`: torch.Tensor with the parameter, useful for implementing weight decay and accessing per-parameter states.
* :code:`grad`: torch.Tensor with the initial gradient, not transformed by previous modules. Useful for things like cautious optimizers that compare update sign with gradient sign. Sometimes gradient is never evaluated, like in gradient free methods, so this may be None.
* per-parameter settings in any order, in the Adam example below :code:`beta1, beta2, eps, alpha`. Everything passed to :code:`defaults` will be accessible there.

The method should return the updated ascent direction tensor. Please do not update :code:`param` directly.

Here is a ready to use Adam implementation through overwriting :code:`_single_tensor_update`:

.. code:: python

    import torch, torchzero as tz

    class Adam(tz.core.OptimizerModule):
        def __init__(self, beta1 = 0.9, beta2 = 0.99, eps = 1e-8, alpha = 1):
            defaults = dict(beta1 = beta1, beta2 = beta2, eps = eps, alpha = alpha)
            super().__init__(defaults)


        def _single_tensor_update(
            self,
            vars,
            ascent:torch.Tensor,
            param:torch.Tensor,
            grad:torch.Tensor | None,
            beta1: float,
            beta2: float,
            eps: float,
            alpha: float,
        ):
            # ininitalize exponential averages in per-parameter state
            state = self.state[param]
            if 'exp_avg' not in state: state['exp_avg'] = torch.zeros_like(param)
            if 'exp_avg_sq' not in state: state['exp_avg_sq'] = torch.zeros_like(param)
            if 'step' not in state: state['step'] = 1

            exp_avg: torch.Tensor = state['exp_avg']
            exp_avg_sq: torch.Tensor = state['exp_avg_sq']
            step = state['step']

            # adam update rule
            exp_avg.lerp_(ascent, 1-beta1)
            exp_avg_sq.mul_(beta2).addcmul_(ascent, ascent, value = 1-beta2)

            bias_correction1 = 1 - beta1**step
            bias_correction2 = 1 - beta2**step

            denom = exp_avg_sq.sqrt().div_(bias_correction2**0.5) + eps

            state['step'] += 1

            # returns updated ascent direction tensor
            return (exp_avg / denom).mul_(alpha / bias_correction1)

    opt = tz.Modular(model.parameters(), tz.m.ClipValue(1), Adam(), tz.m.LR(1e-2))


Method 2. Overwriting _update
+++++++++++++++++++++++++++++++++++++++++++++
:code:`_update` is similar to :code:`_single_tensor_update`, however you get access to all ascent tensors in a single list, as opposed to looping through each element. That way you can use pytorch `_foreach_xxx <https://pytorch.org/docs/stable/torch.html#foreach-operations>`_ operations for better performance. Most modules in torchzero are implemented through overwriting :code:`_update` and with :code:`_foreach` operations.

:code:`update` accepts the following arguments:

* :code:`vars`: :py:mod:`tz.core.OptimizationVars<torchzero.core.OptimizationVars>` object with various useful attributes, such as closure, list of current update tensors, loss, list of gradient tensors. For now we don't need this.
* :code:`ascent`: :py:mod:`tz.TensorList<torchzero.TensorList>` - list of tensors of the ascent direction (gradient or update) for each parameter with :code:`requires_grad = True`. :code:`TensorList` is a subclass of python list with some additional methods, but we won't use those methods for now. As it is a subclass of list, you can pass it directly to :code:`torch._foreach_xxx` methods.

The method should return the updated ascent :code:`TensorList`.

To make working with lists of tensors more convenient, :code:`OptimizerModule` also has some helper methods.

* :code:`self.get_params()`: returns list of tensors of all params with :code:`requires_grad = True`.
* :code:`self.get_group_key(key)`, :code:`self.get_group_keys(*keys)`: return list of values of a per-parameter setting (such as beta1, beta2, eps) for each parameter with :code:`requires_grad = True`.
* :code:`self.get_state_key(key)`, :code:`self.get_state_keys(*keys)`: return a list of tensors of a state (e.g. exponential average) of each parameter with :code:`requires_grad = True`, initializes the state to zeroes if it doesn't exist.

Here is a ready to use Adam implementation through overwriting :code:`_update` using :code:`_foreach` methods. Using a lot of :code:`_foreach_xxx` methods is not very readable, but it is fast.

.. code:: python

    import torch, torchzero as tz

    def _foreach_lerp2_(self: list[torch.Tensor], tensors1: list[torch.Tensor], weight: list[float]):
        """_foreach_lerp_ but supports list of scalars as weight"""
        difference = torch._foreach_sub(tensors1, self)
        torch._foreach_mul_(difference, weight)
        torch._foreach_add_(self, difference)


    class ForeachAdam(tz.core.OptimizerModule):
        def __init__(self, beta1 = 0.9, beta2 = 0.99, eps = 1e-8, alpha = 1):
            defaults = dict(beta1 = beta1, beta2 = beta2, eps = eps, alpha = alpha)
            super().__init__(defaults)
            self.current_step = 1

        def _update(self, vars, ascent:tz.TensorList):
            # get lists of all adam settings per each parameter
            beta1, beta2, eps, alpha = self.get_group_keys('beta1', 'beta2', 'eps', 'alpha')

            # get exponential averages, initialize them to zeros if they haven't been initialized
            exp_avg, exp_avg_sq = self.get_state_keys('exp_avg', 'exp_avg_sq', inits = torch.zeros_like)

            # adam update rule
            _foreach_lerp2_(exp_avg, ascent, [1 - i for i in beta1])
            torch._foreach_mul_(exp_avg_sq, beta2)
            torch._foreach_addcmul_(exp_avg_sq, ascent, ascent, scalars = [1 - i for i in beta2])

            bias_correction1 = [1 - i**self.current_step for i in beta1]
            bias_correction2 = [1 - i**self.current_step for i in beta2]

            denom = torch._foreach_sqrt(exp_avg_sq)
            torch._foreach_div_(denom, [c ** 0.5 for c in bias_correction2])
            torch._foreach_add_(denom, eps)

            ret = torch._foreach_div(exp_avg, denom)
            torch._foreach_mul_(ret, [a/d for a,d in zip(alpha, bias_correction1)])

            # returns updated ascent direction tensorlist (make sure it is a TensorList)
            return tz.TensorList(ret)

    opt = tz.Modular(model.parameters(), tz.m.ClipValue(1), ForeachAdam(), tz.m.LR(1e-2))


Method 3. Overwriting step
+++++++++++++++++++++++++++++++++++++++++++++
:code:`step` method gives you the most control, but it requires the most understanding of the internals of torchzero. You can reevaluate the closure multiple times which is usually necessary for line searches and gradient approximation. You can step with multiple modules, skip an update, update parameters directly, basically anything is possible.

There are also helper classes: :py:mod:`GradientApproximatorBase<tz.modules.gradient_approximation.GradientApproximatorBase>` allows you to define a gradient approximation module in a more convenient way by overwriting :code:`_make_ascent` method. :py:mod:`GradientApproximatorBase<tz.modules.line_search.LineSearchBase>` is an easy way to define line searches by overwriting :code:`_find_best_lr`.

This section is WIP