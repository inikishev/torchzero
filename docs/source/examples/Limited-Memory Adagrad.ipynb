{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edef565",
   "metadata": {},
   "source": [
    "# LM-Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df645d9f",
   "metadata": {},
   "source": [
    "In this notebook I describe a limited-memory version of full matrix Adagrad algorithm. If this algorithm has been already described somewhere else, please let me know.\n",
    "\n",
    "### Full-matrix Adagrad\n",
    "The standard full-matrix Adagrad update rule is:\n",
    "\n",
    "Hyperparameters: learning rate $\\alpha$ and regularization parameter $\\epsilon$\n",
    "\n",
    "To initialize the algorithm, create the accumulation buffer A of shape $d\\times d$, where $d$ is number of parameters, initialized to zeros.\n",
    "\n",
    "On time step $t$ you have parameters $w_t$ and do this:\n",
    "\n",
    "1. Evaluate the gradient vector $g_t$\n",
    "2. Calculate outer product of gradient with itself: $G=g_t g_t^T$\n",
    "3. Update accumulator with gradient outer product: $A_{t+1}=A_{t}+G$\n",
    "4. Make a regularized version of $A_{t+1}$ by adding identity matrix times small value: $A_{reg} = A_{t+1}+I\\cdot\\epsilon$\n",
    "5. Update the parameters: $w_{t+1}=w_{t}-\\alpha \\cdot A_{reg}^{-1/2}g_{t}$\n",
    "\n",
    "The main disadvantage of full-matrix Adagrad is that the matrix $A$ is prohibitively large. That's why authors suggested using just the diagonal of $A$, however that gets rid of a lot of information about off-diagonal elements. Methods like Shampoo and KFAC have been proposed to use a little bit more than just the diagonal of $A$, but not the full matrix.\n",
    "\n",
    "### Derivation of Limited-Memory Adagrad\n",
    "\n",
    "I realized that what Full-matrix Adagrad actually does is it applies ZCA whitening, except it doesn't perform centering. But ZCA whitening is commonly applied to large datasets via singular value decomposition (SVD), which can also be adapted to make a memory efficient version of Adagrad, and here is how:\n",
    "\n",
    "Suppose we take past $k$ gradient vectors and stack them as columns into a single matrix $M\\in \\mathbb{R}^{d\\times k}$. Accumulator $A$ is sum of outer products of columns of $M$, so it can be calculated as $A=MM^T$.\n",
    "\n",
    "Now let's consider a thin SVD of $M$:\n",
    "\n",
    "$M=U\\Sigma V^T$\n",
    "\n",
    "Here $U$ is $d\\times k$, $\\Sigma$ is a diagonal matrix with $k$ singular values, and $V$ is an orthogonal $k\\times k$ matrix.\n",
    "\n",
    "We know that $A=MM^T$, now swap M for it's SVD:\n",
    "\n",
    "$A = MM^T = (U \\Sigma V^T)(U \\Sigma V^T)^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma^2 U^T$\n",
    "\n",
    "Note: because $V$ is orthogonal, $VV^T=VV^{-1}=I$ (identity)\n",
    "\n",
    "In step 4 of Adagrad we need $A^{-1/2}$\n",
    "\n",
    "$A^{-1/2} = (U \\Sigma^2 U^T)^{-1/2} = U (\\Sigma^2)^{-1/2} U^T = U \\Sigma^{-1} U^T$\n",
    "\n",
    "Note: $(U \\Sigma^2 U^T)^{-1/2}$ is equivalent to $U (\\Sigma^2)^{-1/2} U^T$ because $U \\Sigma^2 U^T$ is a matrix diagonalization.\n",
    "\n",
    "So, the update rule is:\n",
    "\n",
    "$w_{t+1} = w_t - A^{-1/2} g_t = w_t - U \\Sigma^{-1} U^T g_t$\n",
    "\n",
    "The proposed method is equivalent to full matrix Adagrad if it only used sum of outer products of last $k$ gradients.\n",
    "\n",
    "### Limited-Memory Adagrad update rule\n",
    "\n",
    "Hyperparameters: learning rate $\\alpha$, history size $k$ and regularization parameter $\\epsilon$. I set $k=10$ and $\\epsilon=1e-6$, if I find better values I will put them there.\n",
    "\n",
    "To initialize the algorithm, initialize an empty list `history` to store past $k$ gradients.\n",
    "\n",
    "on time step $t$ you have parameters $w_t$ and you do this:\n",
    "\n",
    "1. Evaluate the gradient vector $g_t$\n",
    "2. Append $g_t$ at the end of `history`\n",
    "3. If `length(history) > k`, delete first element in `history`, so that it only has last $k$ gradient vectors\n",
    "4. Stack gradients in `history` as columns of matrix $M\\in \\mathbb{R}^{d\\times k}$.\n",
    "    * Optionally center M by subtracting from each row it's mean $\\bar{g}$:\n",
    "    * $M_{centered} = M - \\bar{g}$, where $\\bar{g}$ is a vector with means of each row of $M$.\n",
    "    * this is how ZCA whitening is performed, and it does seem to help but it also is more unstable.\n",
    "5. compute $U$, $\\Sigma$, $V^T$ = SVD(M). We don't need $V^T$ so it can be discarded. We assume $\\Sigma$ is returned as a vector of singular values.\n",
    "6. Add regularization to singular values: $\\Sigma = (\\Sigma^2+\\epsilon)^{1/2}$\n",
    "7. Update the parameters: $w_{t+1} = w_t - U \\Sigma^{-1} U^T g_t$\n",
    "\n",
    "#### Notes:\n",
    "* To implement step 7, first make a temporary variable $Z=(U^Tg)/S$ , then $w_{t+1} = w_t - UZ$ . Cuz if you multiplied $U \\Sigma^{-1} U^T$ first, you would get $d\\times d$ matrix and you're PC would explode.\n",
    "* There are fast SVD methods for tall matrices. For example in pytorch set `U, S, V = torch.linalg.svd(M, solver=\"gesvda\")`, otherwise it will keep freezing.\n",
    "* The reason regularization is calculated as $\\Sigma = (\\Sigma^2+\\epsilon)^{1/2}$ is because if we did $\\Sigma = \\Sigma+\\epsilon$ , that would be equivalent to adding $I\\cdot\\epsilon$ to square root of $A$ instead of $A$ .\n",
    "* Before applying the update rule, all gradients from all layers can be concatenated into a single gradient vector. Alternatively the update rule can be applied separately to each layer, which ignores interactions between layers.\n",
    "\n",
    "\n",
    "### Tips\n",
    "* Momentum makes it way better. This can be done in a few ways:\n",
    "    * in step 7 $g_t$ can be replaced with a momentum buffer like in Adam\n",
    "    * momentum could be applied to the update itself ( $U \\Sigma^{-1} U^T g_t$ ), like in LaProp.\n",
    "* Clip update norm. Or what I found works well is to clip or graft update norm to exponential moving average of of past (unclipped) updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928df8c8",
   "metadata": {},
   "source": [
    "### Reference implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae17ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def limited_memory_adagrad(\n",
    "    w: torch.Tensor, # current parameters vector\n",
    "    g: torch.Tensor, # current gradient vector\n",
    "    history: list[torch.Tensor], # history of past gradients, initialized to an empty list\n",
    "    lr=1e-2, k=10, eps=1e-6, centered=False # hyperparameters\n",
    "):\n",
    "\n",
    "    # update history\n",
    "    history.append(g)\n",
    "    if len(history) > k: del history[0]\n",
    "\n",
    "    # stack history as columns of M\n",
    "    M = torch.stack(history, dim=1) # (d, k)\n",
    "\n",
    "    # optionally apply centering\n",
    "    if centered:\n",
    "        M -= M.mean(1, keepdim=True)\n",
    "\n",
    "    # compute thin SVD, M has to be on CUDA for \"gesvda\" driver\n",
    "    U, S, _ = torch.linalg.svd(M.cuda(), driver=\"gesvda\")\n",
    "    U = U.to(w); S = S.to(w) # move back to weights device\n",
    "\n",
    "    # regularize singular values\n",
    "    S = (S**2 + eps).sqrt()\n",
    "\n",
    "    # compute U S^-1 U^T g\n",
    "    # start with Z = S^-1 U^T g\n",
    "    Z = (U.T @ g) / S\n",
    "\n",
    "    # now update is U@Z\n",
    "    w -= (U @ Z) * lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e3da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, loss = tensor(1.0929, grad_fn=<MseLossBackward0>)\n",
      "10, loss = tensor(0.7662, grad_fn=<MseLossBackward0>)\n",
      "20, loss = tensor(0.6243, grad_fn=<MseLossBackward0>)\n",
      "30, loss = tensor(0.5514, grad_fn=<MseLossBackward0>)\n",
      "40, loss = tensor(0.5425, grad_fn=<MseLossBackward0>)\n",
      "50, loss = tensor(0.5285, grad_fn=<MseLossBackward0>)\n",
      "60, loss = tensor(0.4883, grad_fn=<MseLossBackward0>)\n",
      "70, loss = tensor(0.4897, grad_fn=<MseLossBackward0>)\n",
      "80, loss = tensor(0.5217, grad_fn=<MseLossBackward0>)\n",
      "90, loss = tensor(0.5008, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Of course there is a torchzero implementation too\n",
    "import torchzero as tz\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "model = nn.Sequential(nn.Linear(10, 10), nn.Tanh(), nn.Linear(10, 10))\n",
    "\n",
    "inputs = torch.randn(64, 10)\n",
    "targets = torch.randn(64, 10)\n",
    "\n",
    "opt = tz.Modular(\n",
    "    model.parameters(),\n",
    "    tz.m.LMAdagrad(),\n",
    "    tz.m.WeightDecay(1e-3),\n",
    "    tz.m.LR(1),\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = F.mse_loss(preds, targets)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 10 == 0: print(f'{i}, {loss = }')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
