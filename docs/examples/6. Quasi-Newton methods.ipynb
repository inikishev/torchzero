{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchzero as tz\n",
    "from visualbench import FunctionDescent, test_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e88eac",
   "metadata": {},
   "source": [
    "# Quasi-Newton methods\n",
    "\n",
    "### Introduction\n",
    "Quasi-Newton methods use approximations to second order information, i.e. the hessian matrix. They do not require computing the hessian matrix via autograd, which is useful in the case when hessian is too expensive or not available. Quasi-newton methods are suitable for all kinds of objectives, including non-convex and even non-smooth. The performance on smooth objectives is however typically better, so for example in a neural network you may get better results by replacing ReLU with ELU.\n",
    "\n",
    "There are three main classes of Quasi-Newton methods - full-matrix methods, limited-memory methods, and methods where hessian isn't approximated by a full matrix. \n",
    "\n",
    "The full-matrix methods store the full hessian approximation and thus require $N^2$ memory. Since in pytorch we can use GPU acceleration, full-matrix methods are fast to compute for problems under ~10,000 variables, although that number depends on how good your GPU is. Full-matrix methods usually have the fastest convergence, and they are faster to compute compared to limited-memory methods as long as the CPU/GPU can handle it.\n",
    "\n",
    "Limited-memory methods do not store the full hessian and instead use a history of past parameter and gradient differences, so they are suitable for large scale optimization. \n",
    "\n",
    "Finally some methods maintain a diagonal hessian approximation or even a scalar (as in Barzilaiâ€“Borwein method), so they also do not suffer from $N^2$ memory requirement.\n",
    "\n",
    "### Secant equation\n",
    "Quasi-newton methods are usually based on the secant equation:\n",
    "$$\n",
    "Bs=y\n",
    "$$\n",
    "Here $B$ is hessian approximation, $s=x_t-x_{t-1}$ is difference between parameters, $y=\\nabla f(x_t)-\\nabla f(x_{t-1})$ is difference between gradients. The differences are usually between current and previous step, although it is possible to sample the differences from different points.\n",
    "\n",
    "The secant equation is also defined for hessian inverse, allowing to maintain it directly.\n",
    "$$\n",
    "Hy=s\n",
    "$$\n",
    "Here $H$ is hessian inverse approximation.\n",
    "\n",
    "A note that for some devious reason, when usually $H$ denotes the hessian, in quasi-newton literature $H$ universally denotes **hessian inverse** approximation and $B$ denotes **hessian** approximation.\n",
    "\n",
    "Different quasi-newton methods differ in how they solve the secant equation. Usually it is solved in a way so that $B_{t+1}$ is as close as possible to $B_t$ in some norm.\n",
    "\n",
    "\n",
    "### Quick recommendations:\n",
    "\n",
    "There are a lot of quasi-newton methods in torchzero. However the following are very good baselines:\n",
    "\n",
    "Problems under ~10,000 parameters:\n",
    "```python\n",
    "tz.Modular(\n",
    "    model.parameters(),\n",
    "    tz.m.RestartOnStuck(tz.m.BFGS()),\n",
    "    tz.m.Backtracking(),\n",
    ")\n",
    "```\n",
    "Problems under ~5,000 parameters\n",
    "```python\n",
    "tz.Modular(\n",
    "    model.parameters(),\n",
    "    tz.m.LevenbergMarquardt(\n",
    "        tz.m.RestartOnStuck(tz.m.SR1(inverse=False))\n",
    "    ),\n",
    ")\n",
    "```\n",
    "Large scale optimization (history_size can be increased to 100 if affordable):\n",
    "```python\n",
    "tz.Modular(\n",
    "    model.parameters(),\n",
    "    tz.m.LBFGS(history_size=10),\n",
    "    tz.m.Backtracking(),\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d30f1",
   "metadata": {},
   "source": [
    "### BFGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924a84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
